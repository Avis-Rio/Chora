import os
import sys
import yaml
import json
import re
import requests
from utils.word_count import update_rewritten_file

def load_config():
    config_path = 'config/sources.yaml'
    if not os.path.exists(config_path):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶ {config_path}")
        print(f"è¯·ä» config/sources.example.yaml å¤åˆ¶å¹¶å¡«å…¥ API å¯†é’¥")
        return None
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    if not config:
        print("é”™è¯¯: é…ç½®æ–‡ä»¶ä¸ºç©º")
        return None
    
    return config

def validate_api_config(config):
    """éªŒè¯ API é…ç½®æ˜¯å¦æœ‰æ•ˆ"""
    if not config:
        return False, "é…ç½®ä¸ºç©º"
    
    api_keys = config.get('api_keys', {})
    llm_config = api_keys.get('llm', {})
    api_key = llm_config.get('api_key', '')
    
    if not api_key:
        return False, "LLM API å¯†é’¥æœªé…ç½®"
    
    if 'your_' in api_key or api_key == 'your_llm_api_key_here':
        return False, "LLM API å¯†é’¥æ˜¯å ä½ç¬¦ï¼Œè¯·å¡«å…¥æœ‰æ•ˆå¯†é’¥"
    
    if not llm_config.get('base_url'):
        return False, "LLM base_url æœªé…ç½®"
    
    return True, "é…ç½®æœ‰æ•ˆ"

def read_file(path):
    with open(path, 'r', encoding='utf-8') as f:
        return f.read()

def save_file(path, content):
    with open(path, 'w', encoding='utf-8') as f:
        f.write(content)

def detect_language(text):
    """æ£€æµ‹æ–‡æœ¬æ˜¯å¦ä¸»è¦æ˜¯è‹±æ–‡"""
    # ç»Ÿè®¡ASCIIå­—ç¬¦æ¯”ä¾‹
    ascii_chars = sum(1 for c in text if ord(c) < 128)
    total_chars = len(text)
    if total_chars == 0:
        return 'unknown'
    ratio = ascii_chars / total_chars
    return 'english' if ratio > 0.8 else 'chinese'

def rewrite_content(transcript_path, metadata_path, output_path):
    print(f"Starting rewrite for {transcript_path}...")

    if not os.path.exists(transcript_path):
        print(f"Error: Transcript file not found: {transcript_path}")
        return False

    config = load_config()
    if not config:
        return False
    
    # éªŒè¯ API é…ç½®
    is_valid, message = validate_api_config(config)
    if not is_valid:
        print(f"âŒ API é…ç½®é”™è¯¯: {message}")
        print("è¯·ç¼–è¾‘ config/sources.yaml å¹¶å¡«å…¥æœ‰æ•ˆçš„ API å¯†é’¥")
        return False

    # Read inputs
    transcript = read_file(transcript_path)
    prompt_template = read_file('config/rewrite-prompt.md')
    
    # æ£€æµ‹è¯­è¨€ï¼Œå¦‚æœæ˜¯è‹±æ–‡åˆ™æ·»åŠ ç¿»è¯‘æŒ‡ä»¤
    lang = detect_language(transcript)
    translation_instruction = ""
    if lang == 'english':
        print("ğŸ“ æ£€æµ‹åˆ°è‹±æ–‡è½¬å½•ï¼Œå°†åœ¨æ”¹å†™æ—¶è‡ªåŠ¨ç¿»è¯‘ä¸ºä¸­æ–‡")
        translation_instruction = "\n\n**é‡è¦æç¤ºï¼šåŸæ–‡æ˜¯è‹±æ–‡ï¼Œè¯·åœ¨æ”¹å†™æ—¶å°†å†…å®¹ç¿»è¯‘ä¸ºæµç•…çš„ä¸­æ–‡ã€‚**\n"

    # Read metadata if available
    metadata_context = ""
    if os.path.exists(metadata_path):
        metadata_context = f"\n\nMetadata:\n{read_file(metadata_path)}"

    # Construct prompt for Gemini
    full_prompt = f"""
    {prompt_template}
    {translation_instruction}

    ---

    TRANSCRIPT:
    {transcript}
    {metadata_context}

    ---
    """

    # Gemini API Setup
    llm_config = config['api_keys']['llm']
    # ä½¿ç”¨ Bearer Token è®¤è¯æ–¹å¼ï¼ˆäº‘é›¾APIè¦æ±‚ï¼‰
    base_url = llm_config['base_url']

    # åˆ‡æ¢åˆ°æµå¼ API ç«¯ç‚¹
    if ":generateContent" in base_url:
        url = base_url.replace(":generateContent", ":streamGenerateContent?alt=sse")
    else:
        # Fallback if URL format is unexpected, append stream method
        url = base_url.rstrip('/') + ":streamGenerateContent?alt=sse"

    payload = {
        "contents": [{
            "role": "user",
            "parts": [{"text": full_prompt}]
        }],
        "generationConfig": {
            "temperature": 0.7,
            "topP": 0.95,
            "maxOutputTokens": 65536, # Further increased to prevent truncation
        }
    }

    try:
        print(f"Sending streaming request to Gemini ({llm_config['model']})...")
        print(f"URL: {url}")

        headers = {
            'Content-Type': 'application/json',
            'Authorization': f"Bearer {llm_config['api_key']}"
        }

        # ä½¿ç”¨ requests å‘é€æµå¼è¯·æ±‚
        response = requests.post(
            url,
            json=payload,
            headers=headers,
            timeout=120, # è¿æ¥è¶…æ—¶120ç§’ï¼Œè¯»å–è¶…æ—¶ç”±æµå¼å¤„ç†
            stream=True
        )

        print(f"Response received with status: {response.status_code}")

        if response.status_code != 200:
            print(f"Error: API returned status {response.status_code}")
            print(f"Response: {response.text[:500]}")
            return False

        # Extract text from Gemini streaming response
        rewritten_content = ""
        print("Receiving stream...")

        for line in response.iter_lines():
            if line:
                decoded_line = line.decode('utf-8')
                if decoded_line.startswith('data: '):
                    json_str = decoded_line[6:] # Remove 'data: ' prefix
                    try:
                        chunk = json.loads(json_str)
                        if 'candidates' in chunk and chunk['candidates']:
                            candidate = chunk['candidates'][0]
                            if 'content' in candidate and 'parts' in candidate['content']:
                                for part in candidate['content']['parts']:
                                    # è·³è¿‡ Gemini çš„ thinking å†…å®¹ (thought: true)
                                    if part.get('thought', False):
                                        continue
                                    if 'text' in part:
                                        text_chunk = part['text']
                                        rewritten_content += text_chunk
                                        # Optional: print progress dot
                                        print(".", end="", flush=True)
                    except json.JSONDecodeError:
                        pass

        print("\nStream complete.")

        if not rewritten_content:
            print("Error: No content generated.")
            return False

        # Content completeness validation
        required_sections = ["æ ¸å¿ƒæ´å¯Ÿ", "å“²æ€ç»“è¯­"]
        missing_sections = [s for s in required_sections if s not in rewritten_content]
        if missing_sections:
            print(f"âš ï¸ Warning: Content may be incomplete. Missing sections: {missing_sections}")
            print(f"   Total generated content length: {len(rewritten_content)} characters")

        # Save output

        # 1. Extract Metadata Section
        # Allow missing closing tag to handle cutoff content
        metadata_match = re.search(r'<METADATA_SECTION>(.*?)(?:</METADATA_SECTION>|$)', rewritten_content, re.DOTALL)
        if metadata_match:
            ai_metadata = metadata_match.group(1).strip()

            # Remove any trailing XML tags if they got caught (e.g., <REWRITE_SECTION>)
            ai_metadata = re.sub(r'<REWRITE_SECTION>.*', '', ai_metadata, flags=re.DOTALL).strip()
            
            # ä»ç°æœ‰ metadata.md è¯»å–æ‰€æœ‰åŸå§‹å­—æ®µï¼ˆå¿…é¡»å®Œæ•´ä¿ç•™ï¼‰
            original_fields = {
                'title': '',
                'source': '',
                'source_url': '',
                'publish_date': ''
            }
            
            if os.path.exists(metadata_path):
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    existing_content = f.read()
                    
                    # æå–æ ‡é¢˜ (# å¼€å¤´)
                    for line in existing_content.split('\n'):
                        if line.startswith('# '):
                            original_fields['title'] = line
                            break
                    
                    # æå–æ¥æº
                    source_match = re.search(r'##\s*æ¥æº\s*\n(.+?)(?=\n##|\Z)', existing_content, re.MULTILINE | re.DOTALL)
                    if source_match:
                        original_fields['source'] = source_match.group(1).strip()
                    
                    # æå–åŸå§‹é“¾æ¥
                    url_match = re.search(r'##\s*åŸå§‹é“¾æ¥\s*\n(.+?)(?=\n##|\Z)', existing_content, re.MULTILINE | re.DOTALL)
                    if url_match:
                        original_fields['source_url'] = url_match.group(1).strip()
                    
                    # æå–å‘å¸ƒæ—¶é—´
                    date_match = re.search(r'##\s*å‘å¸ƒæ—¶é—´\s*\n(.+?)(?=\n##|\Z)', existing_content, re.MULTILINE | re.DOTALL)
                    if date_match:
                        original_fields['publish_date'] = date_match.group(1).strip()
            
            # ä» AI è¾“å‡ºä¸­æå–å˜‰å®¾å’Œé‡‘å¥
            ai_guests = ""
            ai_quotes = ""
            
            guests_match = re.search(r'##\s*å˜‰å®¾\s*\n(.+?)(?=\n##|\Z)', ai_metadata, re.MULTILINE | re.DOTALL)
            if guests_match:
                ai_guests = guests_match.group(1).strip()
            
            quotes_match = re.search(r'##\s*é‡‘å¥\s*\n(.+?)(?=\n##|\Z)', ai_metadata, re.MULTILINE | re.DOTALL)
            if quotes_match:
                ai_quotes = quotes_match.group(1).strip()
            
            # æ„å»ºæœ€ç»ˆ metadataï¼ˆæŒ‰æ ‡å‡†æ ¼å¼ï¼‰
            final_metadata = ""
            
            # æ ‡é¢˜
            if original_fields['title']:
                final_metadata += f"{original_fields['title']}\n\n"
            
            # æ¥æº
            if original_fields['source']:
                final_metadata += f"## æ¥æº\n{original_fields['source']}\n\n"
            
            # åŸå§‹é“¾æ¥
            if original_fields['source_url']:
                final_metadata += f"## åŸå§‹é“¾æ¥\n{original_fields['source_url']}\n\n"
            
            # å‘å¸ƒæ—¶é—´
            if original_fields['publish_date']:
                final_metadata += f"## å‘å¸ƒæ—¶é—´\n{original_fields['publish_date']}\n\n"
            
            # å˜‰å®¾ï¼ˆAI ç”Ÿæˆï¼‰
            if ai_guests and ai_guests.lower() not in ['æ— ', '[ä¸»è¦å˜‰å®¾æˆ–æ¼”è®²è€…å§“å']:
                final_metadata += f"## å˜‰å®¾\n{ai_guests}\n\n"
            
            # é‡‘å¥ï¼ˆAI ç”Ÿæˆï¼‰
            if ai_quotes and not ai_quotes.startswith('['):
                final_metadata += f"## é‡‘å¥\n{ai_quotes}\n"

            save_file(metadata_path, final_metadata.strip())
            print(f"Updated metadata saved to {metadata_path}")
        else:
            print("Warning: No <METADATA_SECTION> found in output.")

        # 2. Extract Rewrite Section
        # Allow missing closing tag
        rewrite_match = re.search(r'<REWRITE_SECTION>(.*?)(?:</REWRITE_SECTION>|$)', rewritten_content, re.DOTALL)
        if rewrite_match:
            final_rewrite_content = rewrite_match.group(1).strip()
            save_file(output_path, final_rewrite_content)
            print(f"Rewritten content saved to {output_path}")
        else:
            # Fallback: save everything to rewritten.md if no tags found
            # But try to exclude METADATA_SECTION if it exists
            print("Warning: No <REWRITE_SECTION> found, saving filtered output.")

            filtered_content = re.sub(r'<METADATA_SECTION>.*?</METADATA_SECTION>', '', rewritten_content, flags=re.DOTALL).strip()
            # Also remove just the tags if they remain
            filtered_content = filtered_content.replace('<METADATA_SECTION>', '').replace('</METADATA_SECTION>', '')

            save_file(output_path, filtered_content)
            print(f"Rewritten content saved to {output_path}")

        # Update word count
        try:
            update_rewritten_file(output_path)
        except Exception as wc_e:
            print(f"Warning: Failed to update word count: {wc_e}")

        return True

    except Exception as e:
        print(f"Error during rewrite: {e}")
        return False

if __name__ == "__main__":
    if len(sys.argv) < 4:
        print("Usage: python rewrite_service.py <transcript_path> <metadata_path> <output_path>")
        sys.exit(1)

    rewrite_content(sys.argv[1], sys.argv[2], sys.argv[3])
